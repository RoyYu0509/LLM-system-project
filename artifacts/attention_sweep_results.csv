K_N,Q_N,avg_ms,batch_size,error,head_dim,kernel,min_ms,num_heads,std_ms,tier
512,512,0.7293,1,,64,scaled_dot_prod_attention,0.6459,12,0.1292,Long-hd128-12h-1b-512
1024,1024,1.886,1,,64,scaled_dot_prod_attention,1.8186,12,0.0428,Long-hd128-12h-1b-1k
2048,2048,7.2683,1,,64,scaled_dot_prod_attention,7.2394,12,0.0878,Long-hd128-12h-1b-2k
4096,4096,27.7778,1,,64,scaled_dot_prod_attention,27.6678,12,0.088,Long-hd128-12h-1b-4k
8192,8192,116.6522,1,,64,scaled_dot_prod_attention,116.2996,12,0.2226,Long-hd128-12h-1b-8k
16384,16384,0,1,"CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 5.51 GiB is free. Process 543747 has 6.25 GiB memory in use. Of the allocated memory 6.08 GiB is allocated by PyTorch, and 37.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",64,scaled_dot_prod_attention,0,12,0,Long-hd128-12h-1b-16k
512,512,3.2894,8,,64,scaled_dot_prod_attention,3.2741,12,0.0056,Long-hd128-12h-1b-512
1024,1024,12.9713,8,,64,scaled_dot_prod_attention,12.9278,12,0.0396,Long-hd128-12h-1b-1k
2048,2048,55.1139,8,,64,scaled_dot_prod_attention,55.004,12,0.0672,Long-hd128-12h-1b-2k
4096,4096,0,8,"CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 2.46 GiB is free. Process 543747 has 9.29 GiB memory in use. Of the allocated memory 6.17 GiB is allocated by PyTorch, and 2.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",64,scaled_dot_prod_attention,0,12,0,Long-hd128-12h-1b-4k
8192,8192,0,8,"CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 8.51 GiB is free. Process 543747 has 3.24 GiB memory in use. Of the allocated memory 296.12 MiB is allocated by PyTorch, and 2.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",64,scaled_dot_prod_attention,0,12,0,Long-hd128-12h-1b-8k
16384,16384,0,8,"CUDA out of memory. Tried to allocate 48.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 8.61 GiB is free. Process 543747 has 3.15 GiB memory in use. Of the allocated memory 584.12 MiB is allocated by PyTorch, and 2.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",64,scaled_dot_prod_attention,0,12,0,Long-hd128-12h-1b-16k
512,512,0.6364,1,,128,scaled_dot_prod_attention,0.6236,12,0.012,Long-hd128-12h-1b-512
1024,1024,1.8977,1,,128,scaled_dot_prod_attention,1.8842,12,0.0132,Long-hd128-12h-1b-1k
2048,2048,7.51,1,,128,scaled_dot_prod_attention,7.4745,12,0.0396,Long-hd128-12h-1b-2k
4096,4096,29.6346,1,,128,scaled_dot_prod_attention,29.5166,12,0.0813,Long-hd128-12h-1b-4k
8192,8192,122.0907,1,,128,scaled_dot_prod_attention,121.8315,12,0.1298,Long-hd128-12h-1b-8k
16384,16384,0,1,"CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 4.10 GiB is free. Process 543747 has 7.65 GiB memory in use. Of the allocated memory 6.15 GiB is allocated by PyTorch, and 1.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",128,scaled_dot_prod_attention,0,12,0,Long-hd128-12h-1b-16k
512,512,2.1036,2,,256,scaled_dot_prod_attention,2.0904,24,0.0097,Long-hd128-16h-1b-512
1024,1024,7.9878,2,,256,scaled_dot_prod_attention,7.9437,24,0.0551,Long-hd128-16h-1b-1k
2048,2048,33.3649,2,,256,scaled_dot_prod_attention,33.2299,24,0.0872,Long-hd128-16h-1b-2k
4096,4096,134.2902,2,,256,scaled_dot_prod_attention,133.7573,24,0.2009,Long-hd128-16h-1b-4k
8192,8192,0,2,"CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 4.10 GiB is free. Process 543747 has 7.65 GiB memory in use. Of the allocated memory 6.57 GiB is allocated by PyTorch, and 973.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",256,scaled_dot_prod_attention,0,24,0,Long-hd128-16h-1b-8k
16384,16384,0,2,"CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 10.11 GiB is free. Process 543747 has 1.65 GiB memory in use. Of the allocated memory 1.13 GiB is allocated by PyTorch, and 395.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",256,scaled_dot_prod_attention,0,24,0,Long-hd128-16h-1b-16k
512,512,0.6023,1,,64,vectorized_torch,0.5892,12,0.0128,Long-hd128-12h-1b-512
1024,1024,1.6756,1,,64,vectorized_torch,1.667,12,0.0069,Long-hd128-12h-1b-1k
2048,2048,6.233,1,,64,vectorized_torch,6.2228,12,0.0063,Long-hd128-12h-1b-2k
4096,4096,23.6049,1,,64,vectorized_torch,23.5883,12,0.0097,Long-hd128-12h-1b-4k
8192,8192,97.8315,1,,64,vectorized_torch,97.6996,12,0.0611,Long-hd128-12h-1b-8k
16384,16384,0,1,"CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 5.51 GiB is free. Process 543747 has 6.24 GiB memory in use. Of the allocated memory 6.08 GiB is allocated by PyTorch, and 35.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",64,vectorized_torch,0,12,0,Long-hd128-12h-1b-16k
512,512,3.0771,8,,64,vectorized_torch,3.0698,12,0.0059,Long-hd128-12h-1b-512
1024,1024,11.9576,8,,64,vectorized_torch,11.937,12,0.0393,Long-hd128-12h-1b-1k
2048,2048,46.7879,8,,64,vectorized_torch,46.7369,12,0.1828,Long-hd128-12h-1b-2k
4096,4096,0,8,"CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 2.46 GiB is free. Process 543747 has 9.29 GiB memory in use. Of the allocated memory 6.16 GiB is allocated by PyTorch, and 3.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",64,vectorized_torch,0,12,0,Long-hd128-12h-1b-4k
8192,8192,0,8,"CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 8.51 GiB is free. Process 543747 has 3.24 GiB memory in use. Of the allocated memory 296.12 MiB is allocated by PyTorch, and 2.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",64,vectorized_torch,0,12,0,Long-hd128-12h-1b-8k
16384,16384,0,8,"CUDA out of memory. Tried to allocate 48.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 8.61 GiB is free. Process 543747 has 3.15 GiB memory in use. Of the allocated memory 584.12 MiB is allocated by PyTorch, and 2.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",64,vectorized_torch,0,12,0,Long-hd128-12h-1b-16k
512,512,0.6127,1,,128,vectorized_torch,0.5999,12,0.0131,Long-hd128-12h-1b-512
1024,1024,1.7396,1,,128,vectorized_torch,1.731,12,0.0079,Long-hd128-12h-1b-1k
2048,2048,6.4398,1,,128,vectorized_torch,6.4219,12,0.0382,Long-hd128-12h-1b-2k
4096,4096,25.1795,1,,128,vectorized_torch,25.1608,12,0.0102,Long-hd128-12h-1b-4k
8192,8192,102.0077,1,,128,vectorized_torch,101.9477,12,0.0366,Long-hd128-12h-1b-8k
16384,16384,0,1,"CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 2.61 GiB is free. Process 543747 has 9.15 GiB memory in use. Of the allocated memory 6.15 GiB is allocated by PyTorch, and 2.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",128,vectorized_torch,0,12,0,Long-hd128-12h-1b-16k
512,512,1.9521,2,,256,vectorized_torch,1.9435,24,0.0094,Long-hd128-16h-1b-512
1024,1024,7.3713,2,,256,vectorized_torch,7.3605,24,0.0066,Long-hd128-16h-1b-1k
2048,2048,28.8886,2,,256,vectorized_torch,28.8557,24,0.0306,Long-hd128-16h-1b-2k
4096,4096,114.6211,2,,256,vectorized_torch,114.439,24,0.1839,Long-hd128-16h-1b-4k
8192,8192,0,2,"CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 2.61 GiB is free. Process 543747 has 9.15 GiB memory in use. Of the allocated memory 6.57 GiB is allocated by PyTorch, and 2.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",256,vectorized_torch,0,24,0,Long-hd128-16h-1b-8k
16384,16384,0,2,"CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 8.61 GiB is free. Process 543747 has 3.15 GiB memory in use. Of the allocated memory 1.13 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",256,vectorized_torch,0,24,0,Long-hd128-16h-1b-16k
512,512,0.2259,1,,64,vectorized_torch_compiled,0.2133,12,0.0144,Long-hd128-12h-1b-512
1024,1024,0.6323,1,,64,vectorized_torch_compiled,0.616,12,0.0117,Long-hd128-12h-1b-1k
2048,2048,1.8635,1,,64,vectorized_torch_compiled,1.8426,12,0.0203,Long-hd128-12h-1b-2k
4096,4096,5.6902,1,,64,vectorized_torch_compiled,5.6598,12,0.0199,Long-hd128-12h-1b-4k
8192,8192,28.4492,1,,64,vectorized_torch_compiled,28.281,12,0.0878,Long-hd128-12h-1b-8k
16384,16384,115.0546,1,,64,vectorized_torch_compiled,114.5051,12,0.2302,Long-hd128-12h-1b-16k
512,512,1.007,8,,64,vectorized_torch_compiled,0.9923,12,0.0143,Long-hd128-12h-1b-512
1024,1024,3.1931,8,,64,vectorized_torch_compiled,3.169,12,0.0174,Long-hd128-12h-1b-1k
2048,2048,15.4495,8,,64,vectorized_torch_compiled,15.4127,12,0.0512,Long-hd128-12h-1b-2k
4096,4096,61.7124,8,,64,vectorized_torch_compiled,61.5975,12,0.0933,Long-hd128-12h-1b-4k
8192,8192,0,8,"CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 11.33 GiB is free. Process 543747 has 442.00 MiB memory in use. Of the allocated memory 296.12 MiB is allocated by PyTorch, and 11.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",64,vectorized_torch_compiled,0,12,0,Long-hd128-12h-1b-8k
16384,16384,0,8,"CUDA out of memory. Tried to allocate 48.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 11.04 GiB is free. Process 543747 has 730.00 MiB memory in use. Of the allocated memory 584.12 MiB is allocated by PyTorch, and 11.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",64,vectorized_torch_compiled,0,12,0,Long-hd128-12h-1b-16k
512,512,0.4673,1,,128,vectorized_torch_compiled,0.4547,12,0.0114,Long-hd128-12h-1b-512
1024,1024,0.8929,1,,128,vectorized_torch_compiled,0.8767,12,0.0114,Long-hd128-12h-1b-1k
2048,2048,2.3396,1,,128,vectorized_torch_compiled,2.3176,12,0.0194,Long-hd128-12h-1b-2k
4096,4096,7.958,1,,128,vectorized_torch_compiled,7.8599,12,0.1295,Long-hd128-12h-1b-4k
8192,8192,34.5388,1,,128,vectorized_torch_compiled,34.3391,12,0.1047,Long-hd128-12h-1b-8k
16384,16384,136.0505,1,,128,vectorized_torch_compiled,135.667,12,0.272,Long-hd128-12h-1b-16k
512,512,2.5598,2,,256,vectorized_torch_compiled,2.5459,24,0.0116,Long-hd128-16h-1b-512
1024,1024,6.022,2,,256,vectorized_torch_compiled,5.9991,24,0.0229,Long-hd128-16h-1b-1k
2048,2048,16.2983,2,,256,vectorized_torch_compiled,16.1105,24,0.1863,Long-hd128-16h-1b-2k
4096,4096,53.8963,2,,256,vectorized_torch_compiled,53.3408,24,0.2371,Long-hd128-16h-1b-4k
8192,8192,204.8968,2,,256,vectorized_torch_compiled,203.0438,24,0.6277,Long-hd128-16h-1b-8k
16384,16384,0,2,"CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 11.76 GiB of which 10.48 GiB is free. Process 543747 has 1.28 GiB memory in use. Of the allocated memory 1.13 GiB is allocated by PyTorch, and 11.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",256,vectorized_torch_compiled,0,24,0,Long-hd128-16h-1b-16k
512,512,0.3094,1,,64,flash_attention_triton,0.2965,12,0.0136,Long-hd128-12h-1b-512
1024,1024,0.5226,1,,64,flash_attention_triton,0.5073,12,0.0172,Long-hd128-12h-1b-1k
2048,2048,1.4658,1,,64,flash_attention_triton,1.3621,12,0.1075,Long-hd128-12h-1b-2k
4096,4096,4.8,1,,64,flash_attention_triton,4.7853,12,0.0102,Long-hd128-12h-1b-4k
8192,8192,18.2421,1,,64,flash_attention_triton,18.1597,12,0.0786,Long-hd128-12h-1b-8k
16384,16384,72.3212,1,,64,flash_attention_triton,71.8521,12,0.2252,Long-hd128-12h-1b-16k
512,512,0.8165,8,,64,flash_attention_triton,0.809,12,0.0105,Long-hd128-12h-1b-512
1024,1024,2.4633,8,,64,flash_attention_triton,2.4544,12,0.0092,Long-hd128-12h-1b-1k
2048,2048,9.1564,8,,64,flash_attention_triton,9.1263,12,0.0188,Long-hd128-12h-1b-2k
4096,4096,35.9822,8,,64,flash_attention_triton,35.8473,12,0.0295,Long-hd128-12h-1b-4k
8192,8192,145.4139,8,,64,flash_attention_triton,143.3444,12,0.7312,Long-hd128-12h-1b-8k
16384,16384,582.7256,8,,64,flash_attention_triton,581.9378,12,1.4725,Long-hd128-12h-1b-16k
512,512,0.4205,1,,128,flash_attention_triton,0.4136,12,0.011,Long-hd128-12h-1b-512
1024,1024,0.9783,1,,128,flash_attention_triton,0.9641,12,0.0161,Long-hd128-12h-1b-1k
2048,2048,3.0174,1,,128,flash_attention_triton,3.0049,12,0.0085,Long-hd128-12h-1b-2k
4096,4096,11.4345,1,,128,flash_attention_triton,11.391,12,0.1636,Long-hd128-12h-1b-4k
8192,8192,45.0113,1,,128,flash_attention_triton,44.9815,12,0.0479,Long-hd128-12h-1b-8k
16384,16384,175.9235,1,,128,flash_attention_triton,174.8184,12,0.1852,Long-hd128-12h-1b-16k
512,512,2.6938,2,,256,flash_attention_triton,2.5655,24,0.1205,Long-hd128-16h-1b-512
1024,1024,10.2103,2,,256,flash_attention_triton,10.1965,24,0.0099,Long-hd128-16h-1b-1k
2048,2048,40.0923,2,,256,flash_attention_triton,39.8278,24,0.1265,Long-hd128-16h-1b-2k
4096,4096,160.0305,2,,256,flash_attention_triton,159.8847,24,0.0466,Long-hd128-16h-1b-4k
8192,8192,658.9815,2,,256,flash_attention_triton,644.5455,24,9.5228,Long-hd128-16h-1b-8k
16384,16384,2598.2892,2,,256,flash_attention_triton,2596.6824,24,0.6678,Long-hd128-16h-1b-16k
